{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pdf_scraper_colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFh__W059nSC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "7abb6fb4-2bf8-432a-8962-ce474c87bdc5"
      },
      "source": [
        "!pip install tika"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tika\n",
            "  Downloading https://files.pythonhosted.org/packages/96/07/244fbb9c74c0de8a3745cc9f3f496077a29f6418c7cbd90d68fd799574cb/tika-1.24.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tika) (47.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->tika) (1.24.3)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-cp36-none-any.whl size=32885 sha256=38b6821fb7ad4103b017cb493e36ac0c948a6beddb47fb9e2b8341e125192de7\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/9c/f5/0b1b738442fc2a2862bef95b908b374f8e80215550fb2a8975\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-1.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vachNeR39web"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tika\n",
        "from tika import parser\n",
        "#from pdf_scraper import PdfScraper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8OZFlLW9xIz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "7b35855e-4069-456d-b224-c48552505fcd"
      },
      "source": [
        "#colab stuff\n",
        "from google.colab import files\n",
        "\n",
        "#testing in colab code\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive'\n",
        "os.chdir('gdrive/My Drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRkyOhll92sz"
      },
      "source": [
        "os.chdir('colabdata')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5cpcMtT-uVd"
      },
      "source": [
        "pdf_list = os.listdir()[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BRG9mGzDeiV"
      },
      "source": [
        "pdf_scraper = PdfScraper()\n",
        "df = pdf_scraper.scrape(pdf_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HblEpgYP-Adm"
      },
      "source": [
        "df.to_csv('pdf_extracts.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgiiIwBJ-UZe"
      },
      "source": [
        "#colab\n",
        "files.download('pdf_extracts.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wjve5Y9g_R5u"
      },
      "source": [
        "class PdfScraper:\n",
        "\n",
        "    def clean(self, lines):\n",
        "        #clean\n",
        "        bad = ['Microsoft Word', 'Resource Report: Food Programs', 'Nutrition, Food Security, Food', 'Report Date', 'Created by Zoe Arms', 'Financial Resources', 'DISCLAIMER']\n",
        "        clean_lines = []\n",
        "        for i in range(len(lines)):\n",
        "            good_line = True\n",
        "            for b in bad:\n",
        "                if b in lines[i]:\n",
        "                    good_line = False\n",
        "                elif i > 1 and b == 'DISCLAIMER' and (b in lines[i-1] or b in lines[i-2]):\n",
        "                    good_line = False\n",
        "            if lines[i] == '' or lines[i] == ' ' or lines[i] == '  ' or lines[i] == '     ':\n",
        "                good_line = False\n",
        "            if good_line:\n",
        "                clean_lines.append(lines[i])\n",
        "        return clean_lines\n",
        "    \n",
        "    def get_entries(self, clean_lines):\n",
        "        entries = []\n",
        "        entry = []\n",
        "        for line in clean_lines:\n",
        "            if 'No Cost Share' in line:\n",
        "                entries.append(entry)\n",
        "                entry = []\n",
        "            else:\n",
        "                entry.append(line)\n",
        "        return entries\n",
        "    \n",
        "    def get_data(self, entries):\n",
        "        data = []\n",
        "        search_terms = ['Total Funding:', 'Dept/Agency/Org:','Description:','Website:','Eligibility:']\n",
        "        for entry in entries:\n",
        "            record = []\n",
        "            x = ''\n",
        "            m = 0\n",
        "            for line in entry:\n",
        "                if 'Funding not id' in line:\n",
        "                    record.append(x)\n",
        "                    x = ''\n",
        "                    m += 1\n",
        "                if m < len(search_terms) and search_terms[m] in line:\n",
        "                    record.append(x)\n",
        "                    x = line[len(search_terms[m]):]\n",
        "                    m += 1\n",
        "                else:\n",
        "                    x += line\n",
        "            record.append(x)\n",
        "            data.append(record)\n",
        "        return data\n",
        "    \n",
        "    def make_df(self, data,fp):\n",
        "        sponsors = []\n",
        "        sponsor_types = []\n",
        "        org_urls = []\n",
        "        program_urls = []\n",
        "        text_descrs = []\n",
        "        eligibities = []\n",
        "        program_names = []\n",
        "        total_funds = []\n",
        "        deadlines = []\n",
        "        notes = []\n",
        "        org_cities = []\n",
        "        org_states = []\n",
        "        sources = []\n",
        "        source_last_updates = []\n",
        "        content_types  = []\n",
        "        prog_target_vals = []\n",
        "        cols= ['sponsor', 'sponsor_type', 'org_url', 'program_url', 'text_descr','eligibility', 'program_name', 'total_funds', 'deadline', 'notes', 'org_city', 'org_state', 'source', 'source_last_update', 'content_type', 'prog_target']\n",
        "        target_terms = {'Small Business': 'small_biz', 'Nonprofit': 'nonprofit', 'Large Business': 'all_biz', 'Individuals': 'ind'}\n",
        "        sponsor_terms = {'Federal':'public: federal', 'State': 'public: state', 'Local': 'public: city, public: state', 'Nonprofit': 'private: nonprofit', 'Private': 'private: public_company, private: private_company, private: foundation'}\n",
        "        a = 0\n",
        "        b = 1\n",
        "        c = 2\n",
        "        d = 3\n",
        "        e = 4\n",
        "        f = 5\n",
        "        for entry in data:\n",
        "            sources.append(fp)\n",
        "            source_last_updates.append('')\n",
        "            content_types.append('other')\n",
        "            x = entry[a].split('Deadline: ')\n",
        "            program_names.append(x[0])\n",
        "            total_funds.append(entry[b])\n",
        "            try:\n",
        "                sponsors.append(entry[c])\n",
        "            except:\n",
        "                sponsors.append('')\n",
        "            try:\n",
        "                text_descrs.append(entry[d])\n",
        "            except:\n",
        "                text_descrs.append('')\n",
        "            org_cities.append('')\n",
        "            org_states.append('')\n",
        "            try:\n",
        "                deadlines.append(x[1])\n",
        "            except:\n",
        "                deadlines.append('')\n",
        "            try:\n",
        "                program_urls.append(entry[e])\n",
        "                osplit = entry[e].split('.gov')\n",
        "                if len(osplit) > 1:\n",
        "                    org_url = osplit[0] + '.gov'\n",
        "                    org_urls.append(org_url)\n",
        "                else:\n",
        "                    org_urls.append(entry[e])\n",
        "            except:\n",
        "                program_urls.append('')\n",
        "                org_urls.append('')\n",
        "            try:\n",
        "                y = entry[f].split('Notes: ')\n",
        "                eligibities.append(y[0])\n",
        "                terms = ''\n",
        "                spons = ''\n",
        "                for term in target_terms.keys():\n",
        "                    if term in entry[f]:\n",
        "                        terms += target_terms[term] + ', '\n",
        "                for spon in sponsor_terms.keys():\n",
        "                    if spon in entry[f]:\n",
        "                        spons += sponsor_terms[spon] + ', '\n",
        "                if terms == '':\n",
        "                    terms = 'other'\n",
        "                else:\n",
        "                    terms = terms[:-2]\n",
        "                prog_target_vals.append(terms)\n",
        "                if spons == '':\n",
        "                    spons = 'unknown'\n",
        "                else:\n",
        "                    spons = spons[:-2]\n",
        "                sponsor_types.append(spons)\n",
        "                try:\n",
        "                    notes.append(y[1])\n",
        "                except:\n",
        "                    notes.append('')\n",
        "            except:\n",
        "                eligibities.append('')\n",
        "                notes.append('')\n",
        "                prog_target_vals.append('other')\n",
        "                sponsor_types.append('unknown')\n",
        "        df = pd.DataFrame(list(zip(sponsors,sponsor_types,org_urls,program_urls,text_descrs,eligibities,program_names,total_funds,deadlines,notes,org_cities,org_states,sources,source_last_updates,content_types,prog_target_vals)),columns=cols)\n",
        "        return df\n",
        "    \n",
        "    #call this method with list of paths to pdfs as input, output is a dataframe that can be appended to dfs from other scrapers\n",
        "    def scrape(self, pdf_list):\n",
        "        df = pd.DataFrame()\n",
        "        for fp in pdf_list:\n",
        "            file_data = parser.from_file(fp)\n",
        "            text = file_data['content']\n",
        "            lines = text.split('\\n')\n",
        "            clean_lines = self.clean(lines)\n",
        "            entries = self.get_entries(clean_lines)\n",
        "            data = self.get_data(entries)\n",
        "            df = df.append(self.make_df(data, fp),ignore_index=True)\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}